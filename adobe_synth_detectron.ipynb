{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7JZz776LTEO",
        "outputId": "e509f1f6-9be4-4bc5-a226-9b4d3874a9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[K     |████████████████████████████████| 274 kB 5.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp37-cp37m-linux_x86_64.whl size=44092 sha256=3048e45d58f74203780011d659805b14b6e98e614da003e305226a17ddfb7f1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/f5/10/d00a2bd30928b972790053b5de0c703ca87324f3fead0f2fd9\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.1\n",
            "1.10.0+cu111 True\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyyaml==5.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BD8VoZJVOgFi",
        "outputId": "030a3734-c43c-4184-fb97-1ca3660de5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
            "Collecting detectron2\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/detectron2-0.6%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 789 kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20211023.tar.gz (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.62.3)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting black==21.4b2\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 29.6 MB/s \n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.7.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.3)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.4.4)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.10.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (7.1.2)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 27.1 MB/s \n",
            "\u001b[?25hCollecting regex>=2020.1.8\n",
            "  Downloading regex-2021.11.10-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 34.3 MB/s \n",
            "\u001b[?25hCollecting pathspec<1,>=0.8.1\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (5.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (5.4.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.24)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.12.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.42.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.1)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20211023-py3-none-any.whl size=60947 sha256=167bc3d039369bb87be409ec6201df157ff0b6395c4c97fba01cb2969469bd9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/98/fc/252d62cab6263c719120e06b28f3378af59b52ce7a20e81852\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=8fc4307738ec338da9023af5243c826c67bdc9208105b2cf63c3c1f453d889fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, antlr4-python3-runtime, yacs, typed-ast, regex, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, fvcore, black, detectron2\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed antlr4-python3-runtime-4.8 black-21.4b2 detectron2-0.6+cu101 fvcore-0.1.5.post20211023 hydra-core-1.1.1 iopath-0.1.9 mypy-extensions-0.4.3 omegaconf-2.1.1 pathspec-0.9.0 portalocker-2.3.2 regex-2021.11.10 typed-ast-1.5.1 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kW_uzgOxrHux"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "#from detectron2.structures import BoxMode\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icoWKpITlgUJ",
        "outputId": "7be29109-3dc2-4122-fe1c-cfc41d4f4f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hIH8lOVMJMbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c29572c-57cf-4f41-e8ed-03212a593cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace JSONs/pie/40758.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "# IN_COLAB = False\n",
        "# def mount_data_from_drive():\n",
        "#   try:\n",
        "#     import google.colab\n",
        "#     IN_COLAB = True\n",
        "#   except:\n",
        "#     IN_COLAB = False\n",
        "\n",
        "#   if(IN_COLAB):\n",
        "# from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "# # !unzip -qq '/content/drive/MyDrive/release_ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21.zip'\n",
        "# !unzip -qq '/content/drive/MyDrive/ICPR_ChartCompetition2020_AdobeData.zip'\n",
        "#!unzip -qq '/content/ICPR_ChartCompetition2020_AdobeData/Chart-Images-and-Metadata.zip'  \n",
        "!unzip -qq '/content/ICPR_ChartCompetition2020_AdobeData/Task-level-JSONs.zip' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB0B62jGO4-E",
        "outputId": "991f0122-91aa-45bf-8256-61c9b5a3f95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-9fm03tpm\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-9fm03tpm\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.3)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.9)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.62.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.7.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.5.post20211023)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.1.1)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.1.1)\n",
            "Requirement already satisfied: black==21.4b2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (21.4b2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (3.10.0.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (1.5.1)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (2021.11.10)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (7.1.2)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (0.10.2)\n",
            "Requirement already satisfied: pathspec<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2==0.6) (0.4.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.19.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.8)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.3.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.6) (0.29.24)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2==0.6) (57.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2==0.6) (3.6.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.42.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2==0.6) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.1.1)\n",
            "Building wheels for collected packages: detectron2\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp37-cp37m-linux_x86_64.whl size=5678583 sha256=1ccb5ad707e723a70ed16f45b491693c348c84b4fb9258ba032d22024b945059\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b2m2lcfp/wheels/07/dc/32/0322cb484dbefab8b9366bfedbaff5060ac7d149d69c27ca5d\n",
            "Successfully built detectron2\n",
            "Installing collected packages: detectron2\n",
            "  Attempting uninstall: detectron2\n",
            "    Found existing installation: detectron2 0.6+cu101\n",
            "    Uninstalling detectron2-0.6+cu101:\n",
            "      Successfully uninstalled detectron2-0.6+cu101\n",
            "Successfully installed detectron2-0.6\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QF3SoOstHCvZ"
      },
      "outputs": [],
      "source": [
        "# if(IN_COLAB):\n",
        "#   !unzip -qq '/content/drive/MyDrive/release_ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21.zip'\n",
        "#   downloaded = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRtLzmFSPCOJ",
        "outputId": "e8abf637-6664-42f9-b5a4-f1fe81a9be78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.8/index.html\n",
            "Requirement already satisfied: detectron2 in /usr/local/lib/python3.7/dist-packages (0.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2) (3.2.2)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.9)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.0)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.1.1)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (7.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2) (4.62.3)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.0.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.8.9)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.1.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2) (2.7.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.5.post20211023)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: black==21.4b2 in /usr/local/lib/python3.7/dist-packages (from detectron2) (21.4b2)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from detectron2) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2) (1.3.0)\n",
            "Requirement already satisfied: pathspec<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.9.0)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.10.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.5.1)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (2021.11.10)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (0.4.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (3.10.0.2)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (7.1.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==21.4b2->detectron2) (1.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2) (5.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (5.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2) (4.8)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2) (2.3.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.24)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools>=2.0.2->detectron2) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2) (1.15.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core>=1.1->detectron2) (3.6.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2) (0.37.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install detectron2 -f \\\n",
        "  https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.8/index.html\n",
        "\n",
        "\n",
        "from detectron2.structures import BoxMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "rvUxIDoeIDu2"
      },
      "outputs": [],
      "source": [
        "# BASE_DIR = '/content/ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21/'\n",
        "# image_dir = BASE_DIR+'images/'\n",
        "# json_dir = BASE_DIR + 'annotations_JSON/'\n",
        "\n",
        "BASE_DIR = '/content/' #/ICPR_ChartCompetition2020_AdobeData/\n",
        "image_dir = BASE_DIR+'ICPR/Charts/'\n",
        "json_dir = BASE_DIR + 'JSONs/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "cC2WQfs3nLj5"
      },
      "outputs": [],
      "source": [
        "def check_valid_json(json_data, chart_type):\n",
        "\n",
        "  if(json_data):\n",
        "    if not ('task6'in json_data):\n",
        "      return False\n",
        "    if ('task6'in json_data):\n",
        "      val = json_data['task6']\n",
        "      #print(val)\n",
        "      if(val is None):\n",
        "        return False\n",
        "      else:\n",
        "        val = json_data['task6']['output']['visual elements']\n",
        "        if('bar' in val):\n",
        "          val = json_data['task6']['output']['visual elements']['bar']\n",
        "          for box in val:\n",
        "            if(box['height']<=0):\n",
        "              return False\n",
        "            elif(box['width']<=0):\n",
        "              return False\n",
        "        #   else:\n",
        "        #     return False\n",
        "        # else:\n",
        "        #   return False\n",
        "        \n",
        "        # elif(chart_type == 'vertical_bar'):\n",
        "        #   if('boxplot' in val):\n",
        "        #     return True\n",
        "  return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "QRkKP9kRvz_L"
      },
      "outputs": [],
      "source": [
        "def get_paths_dataframe_box(charts ,root_dir,json_dir):\n",
        "  paths_pd_train = {} \n",
        "  paths_pd_val = {}#pd.DataFrame()\n",
        "  img_list = []\n",
        "  json_list = []\n",
        "  img_path = []\n",
        "  json_path = []\n",
        "  single_bbox_data = []\n",
        "  image_ids = []\n",
        "\n",
        "  model_input = []\n",
        "  for i in charts:\n",
        "    path = root_dir + i\n",
        "    img_list = sorted(os.listdir(path))\n",
        "    json_p = json_dir + i\n",
        "    json_list = sorted(os.listdir(json_p))\n",
        "    for j,k in zip(img_list,json_list):\n",
        "      # fi = image_path\n",
        "      if (j.endswith('png') or j.endswith('jpg') ) and k.endswith('json') :\n",
        "        json_fd = open(json_p + '/' + k)\n",
        "        json_data = json.load(json_fd)\n",
        "        #print(json_p + '/' + k)\n",
        "        if(check_valid_json(json_data, i)):\n",
        "          json_single_path = json_p + '/' + k\n",
        "          img_single_path = path + '/' + j\n",
        "          #if(json_data['task6']!='Null'):\n",
        "          img_path.append(img_single_path)\n",
        "          json_path.append(json_single_path)\n",
        "          json_fd = open(json_single_path)\n",
        "          json_data = json.load(json_fd)\n",
        "          # bboxes = get_bounding_box_gt(json_data)\n",
        "          # if not bboxes:\n",
        "          #   print (json_data['task6']['input']['task1_output']['chart_type'])\n",
        "          #   print(\"no bboc found in file \", json_single_path)\n",
        "          # else:\n",
        "          #   for bb in bboxes:\n",
        "          #     model_input.append([img_single_path, bb, j])\n",
        "          image_ids.append(j)\n",
        "  \n",
        "  total_items = len(img_path)\n",
        "  train_img_path = img_path[0: int(0.8*total_items)]\n",
        "  val_img_path = img_path[int(0.8*total_items):-1]\n",
        "\n",
        "  train_json_path = json_path[0: int(0.8*total_items)]\n",
        "  val_json_path = json_path[int(0.8*total_items):-1]\n",
        "\n",
        "  train_image_ids = image_ids[0: int(0.8*total_items)]\n",
        "  val_image_ids = image_ids[int(0.8*total_items):-1]\n",
        "\n",
        "  paths_pd_train['images'] = train_img_path \n",
        "  paths_pd_train['json'] = train_json_path\n",
        "  paths_pd_train['image_ids'] = train_image_ids\n",
        "\n",
        "  paths_pd_val['images'] = val_img_path \n",
        "  paths_pd_val['json'] = val_json_path\n",
        "  paths_pd_val['image_ids'] = val_image_ids\n",
        "  #paths_pd['input'] = model_input\n",
        "  #print(len(img_path))\n",
        "  #print(len(json_path))\n",
        "  return paths_pd_train,paths_pd_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "4biUojrgu0d5"
      },
      "outputs": [],
      "source": [
        "def get_paths_dataframe_point(charts ,root_dir,json_dir):\n",
        "  paths_pd_train = {} #pd.DataFrame()\n",
        "  img_list = []\n",
        "  json_list = []\n",
        "  img_path = []\n",
        "  json_path = []\n",
        "  single_bbox_data = []\n",
        "  model_input = []\n",
        "  for i in charts:\n",
        "    path = root_dir + i\n",
        "    img_list = sorted(os.listdir(path))\n",
        "    json_p = json_dir + i\n",
        "    json_list = sorted(os.listdir(json_p))\n",
        "    for j,k in zip(img_list,json_list):\n",
        "      # fi = image_path\n",
        "      json_fd = open(json_p + '/' + k)\n",
        "      json_data = json.load(json_fd)\n",
        "      #print(json_p + '/' + k)\n",
        "      \n",
        "      if(check_valid_json(json_data, i)):\n",
        "\n",
        "        json_single_path = json_p + '/' + k\n",
        "        img_single_path = path + '/' + j\n",
        "        #if(json_data['task6']!='Null'):\n",
        "        img_path.append(img_single_path)\n",
        "        json_path.append(json_single_path)\n",
        "        json_fd = open(json_single_path)\n",
        "        json_data = json.load(json_fd)\n",
        "        points = get_points(json_data)\n",
        "        if not points:\n",
        "          print (json_data['task6']['input']['task1_output']['chart_type'])\n",
        "          print(\"no points found in file \", json_single_path)\n",
        "        else:\n",
        "          for point in points:\n",
        "            model_input.append([img_single_path, point, j])\n",
        "  \n",
        "  paths_pd['images'] = img_path \n",
        "  paths_pd['json'] = json_path\n",
        "  paths_pd['input'] = model_input\n",
        "  #print(len(img_path))\n",
        "  #print(len(json_path))\n",
        "  return paths_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "rSEr7HTDxpeq"
      },
      "outputs": [],
      "source": [
        "def get_points(input):\n",
        "  points = []\n",
        "  for each_item in input:\n",
        "    class_no = 0\n",
        "    for point in each_item:\n",
        "      x= point[0]\n",
        "      y = point[1]\n",
        "      class_no += 1\n",
        "      points.append([x,y,class_no])\n",
        "\n",
        "  return points\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "5I-2r8dfuoal"
      },
      "outputs": [],
      "source": [
        "def get_points_gt(input):\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"lines\"):\n",
        "    return get_points(input['task6']['output']['visual elements']['line'])\n",
        "  elif(input['task6']['input']['task1_output']['chart_type'] == \"scatter\"):\n",
        "    return get_points(input['task6']['output']['visual elements']['scatter points'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "qrN6Fc5mr3ss"
      },
      "outputs": [],
      "source": [
        "def get_bb_horizontal_box(input , image_new_size, image_old_size ):\n",
        "  \n",
        "  bb_boxes = []\n",
        "  areas = []\n",
        "  for x in input:\n",
        "    height = x['height']\n",
        "    width = x['width']\n",
        "    xmin = x['x0']\n",
        "    ymin = x['y0']\n",
        "    xmax = xmin + width\n",
        "    ymax = ymin + height\n",
        "       \n",
        "\n",
        "    old_width, old_height = image_old_size[0],image_old_size[1]\n",
        "    new_width, new_height = image_new_size[0],image_new_size[1]\n",
        "    #print(old_width, old_height)\n",
        "    #print(new_width, new_height)\n",
        "    xmin_new = int((xmin/old_width)*new_width)\n",
        "    ymin_new = int((ymin/old_height)*new_height)\n",
        "    xmax_new = int((xmax/old_width)*new_width) +1\n",
        "    ymax_new = int((ymax/old_height)*new_height) + 1\n",
        "\n",
        "    #if(xmin_new==xmax_new or ymin_new==ymax_new):\n",
        "      #print(\"error in bounding box : width {} height {}\".format(xmin_new,xmax_new))\n",
        "      \n",
        "    #print(\" read values\", [xmin,ymin,width,height])\n",
        "    #print([xmin_new,ymin_new,xmax_new,ymax_new])\n",
        "    class_no = 0\n",
        "    bb_boxes.append([xmin_new,ymin_new,xmax_new,ymax_new])\n",
        "    area = (xmax_new-xmin_new) * (ymax_new-ymin_new)\n",
        "    areas.append(area)\n",
        "  \n",
        "  return bb_boxes, areas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "bP2ZPJ7hFSez"
      },
      "outputs": [],
      "source": [
        "def get_bb_vertical_box(input , image_new_size, image_old_size, charttype):\n",
        "  bb_boxes = []\n",
        "  areas = []\n",
        "  #print(\"here in vb\")\n",
        "  element_types = [\"first_quartile\",\"max\", \"median\", \"min\", \"third_quartile\" ]\n",
        "  for element in input:\n",
        "    i = 0\n",
        "    width = 0\n",
        "    height = 0 \n",
        "    if(charttype == \"Horizontal box\"):\n",
        "      width =  element[\"first_quartile\"]['_bb']['x0'] - element[\"third_quartile\"]['_bb']['x0']\n",
        "      height= element[\"third_quartile\"]['_bb']['height']\n",
        "    else:\n",
        "      height =  element[\"first_quartile\"]['_bb']['y0'] - element[\"third_quartile\"]['_bb']['y0']\n",
        "      width = element[\"third_quartile\"]['_bb']['width']\n",
        "\n",
        "    xmin = element[\"third_quartile\"]['_bb']['x0']\n",
        "    ymin = element[\"third_quartile\"]['_bb']['y0']\n",
        "    xmax = xmin + width\n",
        "    ymax = ymin + height\n",
        "\n",
        "    old_width, old_height = image_old_size[0],image_old_size[1]\n",
        "    new_width, new_height = image_new_size[0],image_new_size[1]\n",
        "    xmin_new = (xmin/old_width)*new_width\n",
        "    ymin_new = (ymin/old_height)*new_height\n",
        "    xmax_new = (xmax/old_width)*new_width\n",
        "    ymax_new = (ymax/old_height)*new_height\n",
        "    #print(\" read values\", [xmin,ymin,width,height])\n",
        "    #print([xmin_new,ymin_new,xmax_new,ymax_new])\n",
        "    class_no = 0\n",
        "    bb_boxes.append([xmin_new,ymin_new,xmax_new,ymax_new])\n",
        "    area = (xmax_new-xmin_new) * (ymax_new-ymin_new)\n",
        "    areas.append(area)\n",
        "  \n",
        "  return bb_boxes, areas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "mS1ai3yJD5g9"
      },
      "outputs": [],
      "source": [
        "def get_bounding_box_gt(input, image_new_size, image_old_size):\n",
        "\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"vertical box\"):\n",
        "    return get_bb_vertical_box(input['task6']['output']['visual elements']['boxplots'] , image_new_size, image_old_size, \"vertical box\" )\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"horizontal bar\"):\n",
        "    return get_bb_horizontal_box(input['task6']['output']['visual elements']['bars'], image_new_size, image_old_size)\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"Grouped vertical bar\"):\n",
        "    return get_bb_horizontal_box(input['task6']['output']['visual elements']['bars'], image_new_size, image_old_size)\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"Stacked vertical bar\"):\n",
        "    return get_bb_horizontal_box(input['task6']['output']['visual elements']['bars'], image_new_size, image_old_size)\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"Grouped horizontal bar\"):\n",
        "    return get_bb_horizontal_box(input['task6']['output']['visual elements']['bars'], image_new_size, image_old_size)\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"Stacked horizontal bar\"):\n",
        "    return get_bb_horizontal_box(input['task6']['output']['visual elements']['bars'], image_new_size, image_old_size)\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"Vertical box\"):\n",
        "    return get_bb_vertical_box(input['task6']['output']['visual elements']['boxplots'], image_new_size, image_old_size, \"Vertical box\")\n",
        "  if(input['task6']['input']['task1_output']['chart_type'] == \"Horizontal box\"):\n",
        "    return get_bb_vertical_box(input['task6']['output']['visual elements']['boxplots'], image_new_size, image_old_size , \"Horizontal box\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "Fn_rkZPTpOGx"
      },
      "outputs": [],
      "source": [
        "class ChartDataset(Dataset):\n",
        "  def __init__(self,paths, mode = 'box', image_size =416,transforms = None):\n",
        "    self.image_path= paths['images']\n",
        "    self.image_ids = paths['image_ids']\n",
        "    #self.extracted_data = paths['input']\n",
        "    #print(self.image_path)\n",
        "    self.json_path = paths['json'] \n",
        "    # self.to_tensor = torchvision.transforms.ToTensor()\n",
        "    self.data_len = len(self.image_path)\n",
        "    print(self.data_len)\n",
        "    self.transforms = transforms\n",
        "    self.num_samples = self.data_len\n",
        "    self.mode = mode\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "  #   if(self.mode == 'box'):\n",
        "  #     return self.getitem_box(self,index)\n",
        "  #   if(self.mode == 'point'):\n",
        "  #     return self.getitem_point(self,index)\n",
        "    \n",
        "\n",
        "  # # def getitem_point(index):\n",
        "  #   pass\n",
        "\n",
        "\n",
        "  # def getitem_box(index):\n",
        "    \n",
        "    single_image_name = self.image_path[index]\n",
        "\n",
        "    #print(single_image_name)\n",
        "    \n",
        "    image = Image.open(single_image_name)\n",
        "    image = image.convert(\"RGB\")\n",
        "    # if self.transforms:\n",
        "    #   img_as_img = self.transforms(img_as_img)\n",
        "    # image = cv2.imread(single_image_name,cv2.IMREAD_COLOR)\n",
        "    # image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    # image= image/255.0\n",
        "    image_old_size = image.size\n",
        "    #print(image_old_size)\n",
        "    if self.transforms:\n",
        "      image = self.transforms(image)\n",
        "\n",
        "    image_new_size = image.size()\n",
        "    #print(image_new_size)\n",
        "    #img_height = image.shape[0]\n",
        "\n",
        "    json_fd = open(self.json_path[index])\n",
        "    #print(self.json_path[index])\n",
        "    json_data = json.load(json_fd)\n",
        "    #print(json_data)\n",
        "\n",
        "    bboxes, areas = get_bounding_box_gt(json_data, image_new_size, image_old_size)\n",
        "    \n",
        "    #check later to handle this case\n",
        "    #if not bboxes:\n",
        "    #   print (json_data['task6']['input']['task1_output']['chart_type'])\n",
        "      #print(\"no bbox found in file \", json_single_path)\n",
        "    # else:\n",
        "    #   for bb in bboxes:\n",
        "    #     bboxes.append(bb)\n",
        "\n",
        "    bboxes = torch.as_tensor(bboxes,dtype = torch.float32)\n",
        "    areas = torch.as_tensor(areas,dtype = torch.float32)\n",
        "    labels = torch.ones((bboxes.shape[0],),dtype = torch.int64)\n",
        "    iscrowd = torch.zeros((bboxes.shape[0],),dtype = torch.int64)\n",
        "    #print(self.image_ids[index], index)\n",
        "    target = {}\n",
        "    target['boxes'] = bboxes\n",
        "    target['labels'] = labels\n",
        "    target['area'] = areas\n",
        "    target['image_id'] = torch.tensor([index])\n",
        "    target['iscrowd'] = iscrowd\n",
        "    #print(type(image))\n",
        "    \n",
        "\n",
        "    #image = torchvision.transforms.ToTensor()(image)\n",
        "    \n",
        "    return image, target \n",
        "\n",
        "  def __len__(self):\n",
        "    return self.data_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "MCm5D5DWx1dU"
      },
      "outputs": [],
      "source": [
        "# def get_balloon_dicts(img_dir):\n",
        "#     json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
        "#     with open(json_file) as f:\n",
        "#         imgs_anns = json.load(f)\n",
        "\n",
        "#     dataset_dicts = []\n",
        "#     for _, v in imgs_anns.items():\n",
        "#         record = {}\n",
        "        \n",
        "#         filename = os.path.join(img_dir, v[\"filename\"])\n",
        "#         height, width = cv2.imread(filename).shape[:2]\n",
        "        \n",
        "#         record[\"file_name\"] = filename\n",
        "#         record[\"height\"] = height\n",
        "#         record[\"width\"] = width\n",
        "      \n",
        "#         annos = v[\"regions\"]\n",
        "#         objs = []\n",
        "#         for _, anno in annos.items():\n",
        "#             assert not anno[\"region_attributes\"]\n",
        "#             anno = anno[\"shape_attributes\"]\n",
        "#             px = anno[\"all_points_x\"]\n",
        "#             py = anno[\"all_points_y\"]\n",
        "#             poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "#             #print(\"poly1\", poly)\n",
        "#             poly = list(itertools.chain.from_iterable(poly))\n",
        "#             #print(\"poly2\", poly)\n",
        "#             obj = {\n",
        "#                 \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "#                 \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "#                 \"segmentation\": [poly],\n",
        "#                 \"category_id\": 0,\n",
        "#                 \"iscrowd\": 0\n",
        "#             }\n",
        "#             print(obj[\"segmentation\"])\n",
        "#             objs.append(obj)\n",
        "#         record[\"annotations\"] = objs\n",
        "#         dataset_dicts.append(record)\n",
        "#     return dataset_dicts\n",
        "\n",
        "# from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "# for d in [\"train\"]:\n",
        "#     #DatasetCatalog.register(\"balloon/\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n",
        "#     print(get_balloon_dicts((\"balloon/\" + d)))\n",
        "#     #MetadataCatalog.get(\"balloon/\" + d).set(thing_classes=[\"balloon\"])\n",
        "# #balloon_metadata = MetadataCatalog.get(\"balloon/train\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "WEbQQMinZ6_j"
      },
      "outputs": [],
      "source": [
        "# write a function that loads the dataset into detectron2's standard format\n",
        "\n",
        "def get_charts_dicts(paths):\n",
        "  image_paths = paths['images']\n",
        "  json_paths = paths['json']\n",
        "  image_ids = paths['image_ids']\n",
        "\n",
        "  dataset_dicts = []\n",
        "\n",
        "  for image_path,json_path, image_id in zip(image_paths, json_paths, image_ids):\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert(\"RGB\")\n",
        "    width, height = image.size\n",
        "\n",
        "    json_fd = open(json_path)\n",
        "    json_data = json.load(json_fd)\n",
        "\n",
        "    record = {}\n",
        "    record[\"file_name\"] = image_path\n",
        "    record[\"height\"] = height\n",
        "    record[\"width\"] = width\n",
        "    objs = []\n",
        "    bboxes = get_bounding_box_gt(json_data, image.size, image.size)\n",
        "    for box in bboxes[0]: \n",
        "      poly = [box[0], box[1], box[2], box[1], box[2], box[3], box[0], box[3]]\n",
        "      obj = {\n",
        "          \"bbox\":box,\n",
        "          \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "          \"segmentation\": [poly],\n",
        "          \"category_id\": 0,\n",
        "          \"iscrowd\": 0\n",
        "      }\n",
        "      objs.append(obj)\n",
        "      \n",
        "    record[\"annotations\"] = objs\n",
        "    dataset_dicts.append(record)\n",
        "\n",
        "  return dataset_dicts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "2eMEJd30AAVx"
      },
      "outputs": [],
      "source": [
        "# \"horizontal_bar\", \"vertical_box\" ,\n",
        "box_detector_folders = [ \"vbox\", \"hbox\", \"vGroup\", \"vStack\", \"hGroup\", \"hStack\" ]\n",
        "paths_Train, paths_Val = get_paths_dataframe_box(box_detector_folders,image_dir,json_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "gCeURK7T--fe"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import DatasetCatalog\n",
        "for d in [\"Train\"]:\n",
        "    DatasetCatalog.register(\"Chart_Dataset_\" + d ,  lambda d=d:get_charts_dicts(paths_Train))\n",
        "\n",
        "for d in [\"Val\"]:\n",
        "    DatasetCatalog.register(\"Chart_Dataset_\" + d ,  lambda d=d:get_charts_dicts(paths_Val))\n",
        "\n",
        "from detectron2.data import  MetadataCatalog\n",
        "# for d in [\"train\"]:\n",
        "#     #DatasetCatalog.register(\"balloon/\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n",
        "#     print(get_balloon_dicts((\"balloon/\" + d)))\n",
        "MetadataCatalog.get(\"Chart_Dataset_Val\").set(thing_classes=[\"Bars\"])\n",
        "chart_Test_metadata = MetadataCatalog.get(\"Chart_Dataset_Val\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aY4gqfWEmLd",
        "outputId": "f05e41a3-89f9-4010-d6ba-fd6dca319624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(get_charts_dicts(paths_Train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw-WyJBC9KQ7",
        "outputId": "5576ba87-65ad-4349-e245-b06086bc17eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "cfg = None\n",
        "trainer = None\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "goENCEra_9nW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca58239a-3b53-447f-d9c9-238cadd5dc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 80000)\n",
            "(60000, 80000)\n",
            "\u001b[32m[12/18 20:21:54 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): ResNet(\n",
            "    (stem): BasicStem(\n",
            "      (conv1): Conv2d(\n",
            "        3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "        (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (res2): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res3): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (res4): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (3): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (4): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (5): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(1024, 15, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(1024, 60, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): Res5ROIHeads(\n",
            "    (pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (res5): Sequential(\n",
            "      (0): BottleneckBlock(\n",
            "        (shortcut): Conv2d(\n",
            "          1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "        (conv1): Conv2d(\n",
            "          1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (1): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (2): BottleneckBlock(\n",
            "        (conv1): Conv2d(\n",
            "          2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv2): Conv2d(\n",
            "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "        )\n",
            "        (conv3): Conv2d(\n",
            "          512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=2048, out_features=2, bias=True)\n",
            "      (bbox_pred): Linear(in_features=2048, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[12/18 20:24:06 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 5759 images left.\n",
            "\u001b[32m[12/18 20:24:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[12/18 20:24:06 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[12/18 20:24:07 d2.data.common]: \u001b[0mSerializing 5759 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[12/18 20:24:07 d2.data.common]: \u001b[0mSerialized dataset takes 3.89 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/18 20:24:07 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[12/18 20:24:12 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[12/18 20:26:11 d2.utils.events]: \u001b[0m eta: 1:41:55  iter: 19  total_loss: 273.9  loss_cls: 163.6  loss_box_reg: 40.42  loss_rpn_cls: 6.221  loss_rpn_loc: 26.59  time: 6.0616  data_time: 0.4218  lr: 4.9953e-06  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:28:23 d2.utils.events]: \u001b[0m eta: 1:45:22  iter: 39  total_loss: 34.06  loss_cls: 26.95  loss_box_reg: 1.387  loss_rpn_cls: 2.169  loss_rpn_loc: 2.896  time: 6.3314  data_time: 0.3838  lr: 9.9902e-06  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:30:36 d2.utils.events]: \u001b[0m eta: 1:43:49  iter: 59  total_loss: 5.882  loss_cls: 3.858  loss_box_reg: 0.3181  loss_rpn_cls: 0.5908  loss_rpn_loc: 1.729  time: 6.4486  data_time: 0.3860  lr: 1.4985e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:32:53 d2.utils.events]: \u001b[0m eta: 1:42:22  iter: 79  total_loss: 2.707  loss_cls: 0.4952  loss_box_reg: 0.2501  loss_rpn_cls: 0.5519  loss_rpn_loc: 1.3  time: 6.5453  data_time: 0.3878  lr: 1.998e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:35:09 d2.utils.events]: \u001b[0m eta: 1:40:39  iter: 99  total_loss: 2.2  loss_cls: 0.3876  loss_box_reg: 0.2181  loss_rpn_cls: 0.4457  loss_rpn_loc: 1.164  time: 6.5954  data_time: 0.3846  lr: 2.4975e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:37:23 d2.utils.events]: \u001b[0m eta: 1:38:40  iter: 119  total_loss: 2.029  loss_cls: 0.4706  loss_box_reg: 0.287  loss_rpn_cls: 0.4069  loss_rpn_loc: 0.9773  time: 6.6199  data_time: 0.3866  lr: 2.997e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:39:37 d2.utils.events]: \u001b[0m eta: 1:36:28  iter: 139  total_loss: 4.379  loss_cls: 2.423  loss_box_reg: 0.3518  loss_rpn_cls: 0.4192  loss_rpn_loc: 0.9977  time: 6.6270  data_time: 0.3841  lr: 3.4965e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:41:50 d2.utils.events]: \u001b[0m eta: 1:34:05  iter: 159  total_loss: 4.639  loss_cls: 3.261  loss_box_reg: 0.3517  loss_rpn_cls: 0.3988  loss_rpn_loc: 0.9023  time: 6.6318  data_time: 0.3769  lr: 3.996e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:44:04 d2.utils.events]: \u001b[0m eta: 1:31:49  iter: 179  total_loss: 2.956  loss_cls: 0.6766  loss_box_reg: 0.6309  loss_rpn_cls: 0.3897  loss_rpn_loc: 1.019  time: 6.6375  data_time: 0.3854  lr: 4.4955e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:46:18 d2.utils.events]: \u001b[0m eta: 1:29:41  iter: 199  total_loss: 1.522  loss_cls: 0.2711  loss_box_reg: 0.2105  loss_rpn_cls: 0.3309  loss_rpn_loc: 0.6381  time: 6.6471  data_time: 0.3848  lr: 4.995e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:48:33 d2.utils.events]: \u001b[0m eta: 1:27:28  iter: 219  total_loss: 1.318  loss_cls: 0.262  loss_box_reg: 0.2298  loss_rpn_cls: 0.3087  loss_rpn_loc: 0.4804  time: 6.6537  data_time: 0.3915  lr: 5.4945e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:50:48 d2.utils.events]: \u001b[0m eta: 1:25:15  iter: 239  total_loss: 1.336  loss_cls: 0.2627  loss_box_reg: 0.2603  loss_rpn_cls: 0.3124  loss_rpn_loc: 0.5132  time: 6.6606  data_time: 0.3864  lr: 5.994e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:53:04 d2.utils.events]: \u001b[0m eta: 1:23:02  iter: 259  total_loss: 1.441  loss_cls: 0.3146  loss_box_reg: 0.2648  loss_rpn_cls: 0.3083  loss_rpn_loc: 0.4644  time: 6.6717  data_time: 0.3855  lr: 6.4935e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:55:20 d2.utils.events]: \u001b[0m eta: 1:20:55  iter: 279  total_loss: 1.3  loss_cls: 0.264  loss_box_reg: 0.2951  loss_rpn_cls: 0.2888  loss_rpn_loc: 0.3942  time: 6.6830  data_time: 0.3972  lr: 6.993e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:57:36 d2.utils.events]: \u001b[0m eta: 1:18:42  iter: 299  total_loss: 1.441  loss_cls: 0.3524  loss_box_reg: 0.34  loss_rpn_cls: 0.2921  loss_rpn_loc: 0.4105  time: 6.6889  data_time: 0.3912  lr: 7.4925e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 20:59:50 d2.utils.events]: \u001b[0m eta: 1:16:28  iter: 319  total_loss: 1.262  loss_cls: 0.3018  loss_box_reg: 0.2985  loss_rpn_cls: 0.2941  loss_rpn_loc: 0.3846  time: 6.6900  data_time: 0.3732  lr: 7.992e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:02:06 d2.utils.events]: \u001b[0m eta: 1:14:16  iter: 339  total_loss: 1.214  loss_cls: 0.2502  loss_box_reg: 0.2997  loss_rpn_cls: 0.3047  loss_rpn_loc: 0.3508  time: 6.6964  data_time: 0.3761  lr: 8.4915e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:04:22 d2.utils.events]: \u001b[0m eta: 1:12:06  iter: 359  total_loss: 1.195  loss_cls: 0.2405  loss_box_reg: 0.2878  loss_rpn_cls: 0.3027  loss_rpn_loc: 0.3242  time: 6.7028  data_time: 0.4068  lr: 8.991e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:06:38 d2.utils.events]: \u001b[0m eta: 1:09:55  iter: 379  total_loss: 1.207  loss_cls: 0.2319  loss_box_reg: 0.3285  loss_rpn_cls: 0.305  loss_rpn_loc: 0.3242  time: 6.7089  data_time: 0.3913  lr: 9.4905e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:08:55 d2.utils.events]: \u001b[0m eta: 1:07:45  iter: 399  total_loss: 1.073  loss_cls: 0.2155  loss_box_reg: 0.3126  loss_rpn_cls: 0.2836  loss_rpn_loc: 0.2691  time: 6.7150  data_time: 0.4006  lr: 9.99e-05  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:11:12 d2.utils.events]: \u001b[0m eta: 1:05:34  iter: 419  total_loss: 1.179  loss_cls: 0.2854  loss_box_reg: 0.3422  loss_rpn_cls: 0.27  loss_rpn_loc: 0.2887  time: 6.7201  data_time: 0.3910  lr: 0.0001049  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:13:27 d2.utils.events]: \u001b[0m eta: 1:03:19  iter: 439  total_loss: 1.305  loss_cls: 0.2939  loss_box_reg: 0.4068  loss_rpn_cls: 0.2677  loss_rpn_loc: 0.257  time: 6.7226  data_time: 0.3943  lr: 0.00010989  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:15:42 d2.utils.events]: \u001b[0m eta: 1:01:03  iter: 459  total_loss: 1.159  loss_cls: 0.2191  loss_box_reg: 0.4053  loss_rpn_cls: 0.274  loss_rpn_loc: 0.2827  time: 6.7227  data_time: 0.3895  lr: 0.00011489  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:17:57 d2.utils.events]: \u001b[0m eta: 0:58:48  iter: 479  total_loss: 1.228  loss_cls: 0.2646  loss_box_reg: 0.4349  loss_rpn_cls: 0.2617  loss_rpn_loc: 0.2509  time: 6.7254  data_time: 0.3872  lr: 0.00011988  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:20:14 d2.utils.events]: \u001b[0m eta: 0:56:34  iter: 499  total_loss: 1.165  loss_cls: 0.2567  loss_box_reg: 0.3878  loss_rpn_cls: 0.2382  loss_rpn_loc: 0.2609  time: 6.7296  data_time: 0.3813  lr: 0.00012488  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:22:30 d2.utils.events]: \u001b[0m eta: 0:54:18  iter: 519  total_loss: 1.13  loss_cls: 0.2327  loss_box_reg: 0.3336  loss_rpn_cls: 0.2682  loss_rpn_loc: 0.2748  time: 6.7316  data_time: 0.3849  lr: 0.00012987  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:24:45 d2.utils.events]: \u001b[0m eta: 0:52:03  iter: 539  total_loss: 1.282  loss_cls: 0.2784  loss_box_reg: 0.4226  loss_rpn_cls: 0.2811  loss_rpn_loc: 0.2748  time: 6.7327  data_time: 0.3934  lr: 0.00013487  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:27:00 d2.utils.events]: \u001b[0m eta: 0:49:47  iter: 559  total_loss: 1.327  loss_cls: 0.3311  loss_box_reg: 0.415  loss_rpn_cls: 0.2658  loss_rpn_loc: 0.2523  time: 6.7337  data_time: 0.3896  lr: 0.00013986  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:29:17 d2.utils.events]: \u001b[0m eta: 0:47:32  iter: 579  total_loss: 1.26  loss_cls: 0.2868  loss_box_reg: 0.3846  loss_rpn_cls: 0.2634  loss_rpn_loc: 0.2469  time: 6.7371  data_time: 0.3859  lr: 0.00014486  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:31:33 d2.utils.events]: \u001b[0m eta: 0:45:19  iter: 599  total_loss: 1.08  loss_cls: 0.2634  loss_box_reg: 0.3483  loss_rpn_cls: 0.2536  loss_rpn_loc: 0.2189  time: 6.7392  data_time: 0.3928  lr: 0.00014985  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:33:48 d2.utils.events]: \u001b[0m eta: 0:43:03  iter: 619  total_loss: 1.265  loss_cls: 0.2896  loss_box_reg: 0.4215  loss_rpn_cls: 0.2726  loss_rpn_loc: 0.2574  time: 6.7400  data_time: 0.3925  lr: 0.00015485  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:36:04 d2.utils.events]: \u001b[0m eta: 0:40:47  iter: 639  total_loss: 1.221  loss_cls: 0.2298  loss_box_reg: 0.4602  loss_rpn_cls: 0.2535  loss_rpn_loc: 0.287  time: 6.7413  data_time: 0.3926  lr: 0.00015984  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:38:18 d2.utils.events]: \u001b[0m eta: 0:38:30  iter: 659  total_loss: 1.089  loss_cls: 0.2259  loss_box_reg: 0.3624  loss_rpn_cls: 0.2593  loss_rpn_loc: 0.2309  time: 6.7399  data_time: 0.3926  lr: 0.00016484  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:40:33 d2.utils.events]: \u001b[0m eta: 0:36:14  iter: 679  total_loss: 1.132  loss_cls: 0.2032  loss_box_reg: 0.4337  loss_rpn_cls: 0.2384  loss_rpn_loc: 0.2022  time: 6.7409  data_time: 0.3871  lr: 0.00016983  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:42:50 d2.utils.events]: \u001b[0m eta: 0:33:59  iter: 699  total_loss: 1.143  loss_cls: 0.2174  loss_box_reg: 0.4096  loss_rpn_cls: 0.2499  loss_rpn_loc: 0.2416  time: 6.7446  data_time: 0.4109  lr: 0.00017483  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:45:07 d2.utils.events]: \u001b[0m eta: 0:31:43  iter: 719  total_loss: 1.151  loss_cls: 0.2518  loss_box_reg: 0.4153  loss_rpn_cls: 0.2696  loss_rpn_loc: 0.2417  time: 6.7466  data_time: 0.4110  lr: 0.00017982  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:47:23 d2.utils.events]: \u001b[0m eta: 0:29:28  iter: 739  total_loss: 1.253  loss_cls: 0.3026  loss_box_reg: 0.4175  loss_rpn_cls: 0.2611  loss_rpn_loc: 0.2441  time: 6.7480  data_time: 0.4008  lr: 0.00018482  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:49:39 d2.utils.events]: \u001b[0m eta: 0:27:12  iter: 759  total_loss: 1.149  loss_cls: 0.2987  loss_box_reg: 0.3913  loss_rpn_cls: 0.2428  loss_rpn_loc: 0.2341  time: 6.7496  data_time: 0.3995  lr: 0.00018981  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:51:55 d2.utils.events]: \u001b[0m eta: 0:24:56  iter: 779  total_loss: 1.216  loss_cls: 0.234  loss_box_reg: 0.4374  loss_rpn_cls: 0.2496  loss_rpn_loc: 0.2617  time: 6.7507  data_time: 0.3972  lr: 0.00019481  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:54:10 d2.utils.events]: \u001b[0m eta: 0:22:40  iter: 799  total_loss: 1.023  loss_cls: 0.227  loss_box_reg: 0.3622  loss_rpn_cls: 0.2265  loss_rpn_loc: 0.1989  time: 6.7510  data_time: 0.3974  lr: 0.0001998  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:56:27 d2.utils.events]: \u001b[0m eta: 0:20:24  iter: 819  total_loss: 1.089  loss_cls: 0.2328  loss_box_reg: 0.3898  loss_rpn_cls: 0.2423  loss_rpn_loc: 0.2066  time: 6.7529  data_time: 0.3991  lr: 0.0002048  max_mem: 8047M\n",
            "\u001b[32m[12/18 21:58:43 d2.utils.events]: \u001b[0m eta: 0:18:08  iter: 839  total_loss: 1.161  loss_cls: 0.2444  loss_box_reg: 0.384  loss_rpn_cls: 0.2402  loss_rpn_loc: 0.2183  time: 6.7544  data_time: 0.4020  lr: 0.00020979  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:00:59 d2.utils.events]: \u001b[0m eta: 0:15:52  iter: 859  total_loss: 1.112  loss_cls: 0.2442  loss_box_reg: 0.3626  loss_rpn_cls: 0.2415  loss_rpn_loc: 0.2292  time: 6.7552  data_time: 0.4033  lr: 0.00021479  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:03:16 d2.utils.events]: \u001b[0m eta: 0:13:36  iter: 879  total_loss: 1.16  loss_cls: 0.2513  loss_box_reg: 0.4176  loss_rpn_cls: 0.264  loss_rpn_loc: 0.2404  time: 6.7573  data_time: 0.4112  lr: 0.00021978  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:05:32 d2.utils.events]: \u001b[0m eta: 0:11:20  iter: 899  total_loss: 0.9968  loss_cls: 0.2408  loss_box_reg: 0.3483  loss_rpn_cls: 0.2239  loss_rpn_loc: 0.1919  time: 6.7579  data_time: 0.4024  lr: 0.00022478  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:07:48 d2.utils.events]: \u001b[0m eta: 0:09:04  iter: 919  total_loss: 1.12  loss_cls: 0.2537  loss_box_reg: 0.3893  loss_rpn_cls: 0.2635  loss_rpn_loc: 0.2082  time: 6.7592  data_time: 0.3981  lr: 0.00022977  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:10:04 d2.utils.events]: \u001b[0m eta: 0:06:48  iter: 939  total_loss: 1.095  loss_cls: 0.2332  loss_box_reg: 0.3946  loss_rpn_cls: 0.2265  loss_rpn_loc: 0.2094  time: 6.7600  data_time: 0.3933  lr: 0.00023477  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:12:19 d2.utils.events]: \u001b[0m eta: 0:04:32  iter: 959  total_loss: 0.9892  loss_cls: 0.2113  loss_box_reg: 0.3893  loss_rpn_cls: 0.2235  loss_rpn_loc: 0.1828  time: 6.7597  data_time: 0.3915  lr: 0.00023976  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:14:33 d2.utils.events]: \u001b[0m eta: 0:02:16  iter: 979  total_loss: 1.184  loss_cls: 0.2768  loss_box_reg: 0.4048  loss_rpn_cls: 0.2288  loss_rpn_loc: 0.2058  time: 6.7591  data_time: 0.3782  lr: 0.00024476  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:16:48 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 999  total_loss: 1.176  loss_cls: 0.2773  loss_box_reg: 0.4131  loss_rpn_cls: 0.2367  loss_rpn_loc: 0.2281  time: 6.7579  data_time: 0.3749  lr: 0.00024975  max_mem: 8047M\n",
            "\u001b[32m[12/18 22:16:49 d2.engine.hooks]: \u001b[0mOverall training speed: 998 iterations in 1:52:24 (6.7580 s / it)\n",
            "\u001b[32m[12/18 22:16:49 d2.engine.hooks]: \u001b[0mTotal training time: 1:52:26 (0:00:02 on hooks)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-1cf6f64f90c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#trainer.resume_or_load(resume=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \"\"\"\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             assert hasattr(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbefore_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mafter_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/hooks.py\u001b[0m in \u001b[0;36mafter_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;31m# This condition is to prevent the eval from running after a failed training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;31m# func is likely a closure that holds reference to the trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# therefore we clean it to avoid circular reference in the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/hooks.py\u001b[0m in \u001b[0;36m_do_eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtest_and_save_results\u001b[0;34m()\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtest_and_save_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_eval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_eval_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(cls, cfg, model, evaluators)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASETS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0;31m# When evaluators are passed in as arguments,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;31m# implicitly assume that evaluators can be created before data_loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mbuild_test_loader\u001b[0;34m(cls, cfg, dataset_name)\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mOverwrite\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0myou\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0md\u001b[0m \u001b[0mlike\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \"\"\"\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_detection_test_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/config/config.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_called_with_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                     \u001b[0mexplicit_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_args_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0morig_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mexplicit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/config/config.py\u001b[0m in \u001b[0;36m_get_args_from_config\u001b[0;34m(from_config_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupported_arg_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mextra_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_config_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;31m# forward the other arguments to __init__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/build.py\u001b[0m in \u001b[0;36m_test_loader_from_config\u001b[0;34m(cfg, dataset_name, mapper)\u001b[0m\n\u001b[1;32m    454\u001b[0m         ]\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOAD_PROPOSALS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmapper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/build.py\u001b[0m in \u001b[0;36mget_detection_dataset_dicts\u001b[0;34m(names, filter_empty, min_keypoints, proposal_files, check_consistency)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mdataset_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dataset '{}' is empty!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/build.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mdataset_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dataset '{}' is empty!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/catalog.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 )\n\u001b[1;32m     57\u001b[0m             ) from e\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/datasets/coco.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;31m# 1. register a function which returns dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m     \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_coco_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;31m# 2. Optionally, add metadata about this dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/detectron2/data/datasets/coco.py\u001b[0m in \u001b[0;36mload_coco_json\u001b[0;34m(json_file, image_root, dataset_name, extra_annotation_keys)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mcoco_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading {} takes {:.2f} seconds.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/coco/annotations/instances_val2017.json'"
          ]
        }
      ],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_C4_1x.yaml\"))\n",
        "#cfg.MASK_ON = False\n",
        "#print(cfg)\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"Chart_Dataset_Train\",)\n",
        "# cfg.DATASETS.TEST = (\"Chart_Dataset_Val\",)   # no metrics implemented for this dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "#TBD\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "#cfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_   /137849600/model_final_f10217.pkl\"  # initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 10\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000    # 300 iterations seems good enough, but you can certainly train longer\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\n",
        "print(cfg.SOLVER.STEPS)\n",
        "print(cfg.SOLVER.STEPS)\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg) \n",
        "#trainer.resume_or_load(resume=True)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAqRkXMli9pp"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w5CsDo12dTy"
      },
      "outputs": [],
      "source": [
        "#test_image = cv2.imread('/content/ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21/images/horizontal_bar/PMC1449883___pgen.0020024.g008', cv2.IMREAD_COLOR)\n",
        "image_name = \"/content/ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21/images/horizontal_bar/PMC1635563___1471-2458-6-267-2.jpg\"\n",
        "img = cv2.imread(image_name, cv2.IMREAD_COLOR)\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img)\n",
        "output = predictor(img)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1C0-6J7Hd40"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC_ApJK1ha8j"
      },
      "outputs": [],
      "source": [
        "pred_path = '/content/drive/MyDrive/pred_output/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwbYPIIaSnMA"
      },
      "source": [
        "#Final Json Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAREfrKEF-Rm"
      },
      "outputs": [],
      "source": [
        "# image_name = \"/content/ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21/images/horizontal_bar/PMC1635563___1471-2458-6-267-2.jpg\"\n",
        "def build_test_json(test_images, pred_path):\n",
        "\n",
        "  for image_name in test_images:\n",
        "    img = cv2.imread(image_name, cv2.IMREAD_COLOR)\n",
        "    output = predictor(img)\n",
        "    imu = image_name.split('/')\n",
        "    chart_type = imu[-2]\n",
        "    image_na = imu[-1]\n",
        "    dirName = pred_path + chart_type\n",
        "    #print(dirName)\n",
        "    if not os.path.exists(dirName):\n",
        "      os.makedirs(dirName)\n",
        "    image_na_split = image_na.split('.')\n",
        "    image_na_split_right = image_na_split[0]\n",
        "    final_json = dirName + '/' + image_na_split_right + '.json'\n",
        "    x = output['instances'].pred_boxes\n",
        "    a=x.tensor.cpu()\n",
        "    a=a.numpy()\n",
        "    input = {}\n",
        "    input['task6'] = {}\n",
        "    input['task6']['output'] ={}\n",
        "    input['task6']['output']['visual elements'] = {}\n",
        "    input['task6']['output']['visual elements']['bars']=[]\n",
        "    for i in a:\n",
        "      main_box = {}\n",
        "      main_box['height'] = int(i[3])-int(i[1])\n",
        "      main_box['width'] = int(i[2])-int(i[0])\n",
        "      main_box['x0'] = int(i[0])\n",
        "      main_box['y0'] = int(i[1])\n",
        "      input['task6']['output']['visual elements']['bars'].append(main_box)\n",
        "\n",
        "    with open(final_json, 'w') as f:\n",
        "        json.dump(input, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcpZNa_2eiYJ"
      },
      "outputs": [],
      "source": [
        "build_test_json(paths_Val[\"images\"], pred_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqpvjQDrX5s0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "import itertools\n",
        "import editdistance\n",
        "import numpy as np\n",
        "import scipy.optimize\n",
        "import scipy.spatial.distance\n",
        "\n",
        "def check_groups(ds):\n",
        "    try:\n",
        "        _i = ds[0][0]\n",
        "        return 1\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def pprint(obj):\n",
        "    print(json.dumps(obj, indent=4, sort_keys=True))\n",
        "\n",
        "def get_dataseries(json_obj):\n",
        "    if 'task6_output' in json_obj:\n",
        "        return json_obj['task6_output']['visual elements']\n",
        "    elif 'task6' in json_obj:\n",
        "        return json_obj['task6']['output']['visual elements']\n",
        "    return None\n",
        "\n",
        "def euclid(p1, p2):\n",
        "    x1 = float(p1['x'])\n",
        "    y1 = float(p1['y'])\n",
        "    x2 = float(p2['x'])\n",
        "    y2 = float(p2['y'])\n",
        "    return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
        "\n",
        "# def box_to_discrete(ds):\n",
        "#     out = []\n",
        "#     for it_name in ['first_quartile', 'max', 'min', 'median', 'third_quartile']: \n",
        "#         out.append( {'name': it_name, 'x': ds[it_name]['x'], 'y': ds[it_name]['y']} )\n",
        "#     return out\n",
        "\n",
        "def box_arr_to_np(ds):\n",
        "    n = np.zeros( (1, 8))\n",
        "    cnt_q = 0\n",
        "    for _i,p in enumerate(ds):      \n",
        "        n[0,cnt_q] = float(ds[p]['y'])\n",
        "        n[0,cnt_q+1] = float(ds[p]['x'])\n",
        "        cnt_q = cnt_q+1\n",
        "    return n\n",
        "\n",
        "def compare_box(pred_ds, gt_ds, min_dim):\n",
        "    pred_ds = box_arr_to_np(pred_ds)\n",
        "    gt_ds = box_arr_to_np(gt_ds)\n",
        "    cost_mat = np.minimum(1, scipy.spatial.distance.cdist(pred_ds, gt_ds, metric='cityblock') /(min_dim*0.05))\n",
        "    return cost_mat\n",
        "\n",
        "def scatt_arr_to_np(ds):\n",
        "    n = np.zeros((len(ds), 2))\n",
        "    for i, p in enumerate(ds):\n",
        "        n[i,0] = float(p['x'])\n",
        "        n[i,1] = float(p['y'])\n",
        "    return n\n",
        "\n",
        "def bar_arr_to_np(ds):\n",
        "    n = np.zeros([1,4])\n",
        "    n[0,0] = float(ds['y0'])\n",
        "    n[0,1] = float(ds['x0'])\n",
        "    n[0,2] = float(ds['height']) + float(ds['y0'])\n",
        "    n[0,3] = float(ds['width']) + float(ds['x0'])\n",
        "    return n\n",
        "\n",
        "def compare_bar(pred_ds, gt_ds, min_dim):\n",
        "    pred_ds = bar_arr_to_np(pred_ds)\n",
        "    gt_ds = bar_arr_to_np(gt_ds)\n",
        "    \n",
        "    cost_mat = np.minimum(1, scipy.spatial.distance.cdist(pred_ds, gt_ds, metric='cityblock') /(min_dim*0.05))\n",
        "    return cost_mat\n",
        "\n",
        "def compare_scatter(pred_ds, gt_ds, min_dim, gamma, beta):\n",
        "\n",
        "    is_grouped = check_groups(gt_ds)\n",
        "    \n",
        "    if is_grouped:\n",
        "        len_seq = len(gt_ds)\n",
        "    else:\n",
        "        len_seq = 1\n",
        "        pred_ds = [pred_ds]\n",
        "        gt_ds = [gt_ds]\n",
        "\n",
        "    score = np.zeros((len(gt_ds), len(pred_ds)))\n",
        "    for iter_seq1 in range(len(gt_ds)):\n",
        "        gt_seq = scatt_arr_to_np(gt_ds[iter_seq1])\n",
        "\n",
        "        for iter_seq2 in range(len(pred_ds)):\n",
        "            pred_seq = scatt_arr_to_np(pred_ds[iter_seq2])\n",
        "        \n",
        "            # V = np.cov(gt_ds.T)\n",
        "            # VI = np.linalg.inv(V).T\n",
        "            \n",
        "            #cost_mat = np.minimum(1, scipy.spatial.distance.cdist(pred_ds, gt_ds, metric='mahalanobis', VI=VI) / gamma)\n",
        "            cost_mat = np.minimum(1, scipy.spatial.distance.cdist(pred_seq, gt_seq, metric='euclidean') / (min_dim*gamma))\n",
        "        \n",
        "            score[iter_seq1, iter_seq2] = get_score(cost_mat)\n",
        "\n",
        "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-score)\n",
        "    score = score[row_ind, col_ind].sum()/(float(len(gt_ds))*beta)\n",
        "\n",
        "    return score\n",
        "\n",
        "def get_score(cost_mat):\n",
        "    cost_mat = pad_mat(cost_mat)\n",
        "    k = cost_mat.shape[0]\n",
        "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(cost_mat)\n",
        "    \n",
        "    cost = cost_mat[row_ind, col_ind].sum()\n",
        "    score = 1 - (cost / k)\n",
        "    return score\n",
        "\n",
        "def get_cont_recall(p_xs, p_ys, g_xs, g_ys, epsilon):\n",
        "    total_score = 0\n",
        "    total_interval = 0\n",
        "\n",
        "    for i in range(g_xs.shape[0]):\n",
        "        x = g_xs[i]\n",
        "        \n",
        "        if g_xs.shape[0] == 1:\n",
        "            interval = 1\n",
        "        elif i == 0:\n",
        "            interval = (g_xs[i+1] - x) / 2\n",
        "        elif i == (g_xs.shape[0] - 1):\n",
        "            interval = (x - g_xs[i-1]) / 2\n",
        "        else:\n",
        "            interval = (g_xs[i+1] - g_xs[i-1]) / 2\n",
        "\n",
        "        y = g_ys[i]\n",
        "        y_interp = np.interp(x, p_xs, p_ys)\n",
        "        error = min(1, abs( (y - y_interp) / (abs(y) + epsilon)))\n",
        "        total_score += (1 - error) * interval\n",
        "        total_interval += interval\n",
        "\n",
        "    if g_xs.shape[0] != 1:\n",
        "        assert np.isclose(total_interval, g_xs[-1] - g_xs[0])\n",
        "    return total_score / total_interval\n",
        "\n",
        "def compare_continuous(pred_ds, gt_ds):\n",
        "    pred_ds = sorted(pred_ds, key=lambda p: float(p['x']))\n",
        "    gt_ds = sorted(gt_ds, key=lambda p: float(p['x']))\n",
        "\n",
        "    if not pred_ds and not gt_ds:\n",
        "        # empty matches empty\n",
        "        return 1.0\n",
        "    elif not pred_ds and gt_ds:\n",
        "        # empty does not match non-empty\n",
        "        return 0.0\n",
        "    elif pred_ds and not gt_ds:\n",
        "        # empty does not match non-empty\n",
        "        return 0.0\n",
        "\n",
        "    p_xs = np.array([float(ds['x']) for ds in pred_ds])\n",
        "    p_ys = np.array([float(ds['y']) for ds in pred_ds])\n",
        "    g_xs = np.array([float(ds['x']) for ds in gt_ds])\n",
        "    g_ys = np.array([float(ds['y']) for ds in gt_ds])\n",
        "\n",
        "    epsilon = (g_ys.max() - g_ys.min()) / 100.\n",
        "    recall = get_cont_recall(p_xs, p_ys, g_xs, g_ys, epsilon)\n",
        "    precision = get_cont_recall(g_xs, g_ys, p_xs, p_ys, epsilon)\n",
        "\n",
        "    return (2 * precision * recall) / (precision + recall) if (precision + recall) else 0.\n",
        "\n",
        "def norm_edit_dist(s1, s2):\n",
        "    return editdistance.eval(s1, s2) / float(max(len(s1), len(s2), 1))\n",
        "\n",
        "def create_dist_mat(pred_seq, gt_seq, compare, beta):\n",
        "    is_grouped = check_groups(gt_seq)\n",
        "\n",
        "    if not is_grouped:\n",
        "        len_seq = 1\n",
        "        gt_seq = [gt_seq]\n",
        "        pred_seq = [pred_seq]\n",
        "\n",
        "    score = 0\n",
        "    for iter_seq1 in range(len(gt_seq)):\n",
        "        l1 = len(gt_seq[iter_seq1])\n",
        "        tmp_score = 0\n",
        "        \n",
        "        for iter_seq2 in range(len(pred_seq)):\n",
        "            l2 = len(pred_seq[iter_seq2])\n",
        "            mat = np.full( (l1, l2), -1.)\n",
        "            for i in range(l1):\n",
        "                for j in range(l2):\n",
        "                    mat[i,j] = compare(gt_seq[iter_seq1][i], pred_seq[iter_seq2][j])\n",
        "            tmp_score = max(tmp_score, get_score(1 - (mat/beta)))\n",
        "        score += tmp_score\n",
        "    score = score/float(len(gt_seq))\n",
        "\n",
        "    return score\n",
        "\n",
        "def compare_line(pred_ds, gt_ds):\n",
        "    is_grouped = check_groups(gt_ds)\n",
        "    if is_grouped:\n",
        "        score = np.zeros((len(gt_ds), len(pred_ds)))\n",
        "        for iter_seq1 in range(len(gt_ds)):\n",
        "            for iter_seq2 in range(len(pred_ds)):\n",
        "                score[iter_seq1, iter_seq2] = compare_continuous(gt_ds[iter_seq1], pred_ds[iter_seq2])\n",
        "        \n",
        "        row_ind, col_ind = scipy.optimize.linear_sum_assignment(-score)\n",
        "        score = score[row_ind, col_ind].sum()/len(gt_ds)\n",
        "    else:\n",
        "        print(gt_ds)\n",
        "        score = compare_continuous(pred_ds, gt_ds)\n",
        "\n",
        "    return score\n",
        "\n",
        "def pad_mat(mat):\n",
        "    h,w = mat.shape\n",
        "    if h == w:\n",
        "        return mat\n",
        "    elif h > w:\n",
        "        new_mat = np.ones( (h, h) )\n",
        "        new_mat[:,:w] = mat\n",
        "        return new_mat\n",
        "    else:\n",
        "        new_mat = np.ones( (w, w) )\n",
        "        new_mat[:h,:] = mat\n",
        "        return new_mat\n",
        "\n",
        "\n",
        "def metric_6a(pred_data_series, gt_data_series, gt_type, alpha=1, beta=2, gamma=1, img_dim = [1280.0, 960.0], debug=False):\n",
        "    if 'box' in gt_type.lower():\n",
        "        compare = lambda ds1, ds2: compare_box(ds1, ds2, min(img_dim))\n",
        "        pred_no_names = pred_data_series['boxplots']\n",
        "        gt_no_names = gt_data_series['boxplots']\n",
        "        ds_match_score = create_dist_mat(pred_no_names, gt_no_names, compare, beta)\n",
        "    elif 'bar' in gt_type.lower():\n",
        "        compare = lambda ds1, ds2: compare_bar(ds1, ds2, min(img_dim))\n",
        "        pred_no_names = pred_data_series['bars']\n",
        "        gt_no_names = gt_data_series['bars']\n",
        "        ds_match_score = create_dist_mat(pred_no_names, gt_no_names, compare, beta)\n",
        "    elif 'scatter' in gt_type.lower():\n",
        "        pred_no_names = pred_data_series['scatter points']\n",
        "        gt_no_names = gt_data_series['scatter points']\n",
        "        ds_match_score = compare_scatter(pred_no_names, gt_no_names, min(img_dim), gamma, beta)\n",
        "    elif 'line' in gt_type.lower():\n",
        "        pred_no_names = pred_data_series['lines']\n",
        "        gt_no_names = gt_data_series['lines']\n",
        "        ds_match_score = compare_line(pred_no_names, gt_no_names)\n",
        "    else:\n",
        "        raise Exception(\"Odd Case\")\n",
        "  \n",
        "    return ds_match_score\n",
        "\n",
        "def calculate_score(pred_infile, gt_infile):\n",
        "    # if len(sys.argv) < 3:\n",
        "    #     print(\"USAGE: python metric6a.py pred_file|pred_dir gt_file|gt_dir [alpha] [beta] [gamma] [img_dim] [debug]\")\n",
        "    #     exit()\n",
        "    # pred_infile = sys.argv[1]\n",
        "    # gt_infile = sys.argv[2]\n",
        "\n",
        "    try:\n",
        "        alpha = float(sys.argv[3])\n",
        "    except:\n",
        "        alpha = 1\n",
        "    try:\n",
        "        beta = float(sys.argv[4])\n",
        "    except:\n",
        "        beta = 1\n",
        "    try:\n",
        "        gamma = float(sys.argv[5])\n",
        "    except:\n",
        "        gamma = 1    \n",
        "    try:\n",
        "        img_dim = sys.argv[6]\n",
        "    except:\n",
        "        img_dim = [1280, 960.0]\n",
        "    try:\n",
        "        debug = sys.argv[7]\n",
        "    except:\n",
        "        debug = False\n",
        "\n",
        "    if os.path.isfile(pred_infile) and os.path.isfile(gt_infile):\n",
        "        pred_json = json.load(open(pred_infile))\n",
        "        gt_json = json.load(open(gt_infile))\n",
        "\n",
        "        pred_outputs = get_dataseries(pred_json)\n",
        "        gt_outputs = get_dataseries(gt_json)\n",
        "        gt_type = gt_json['task1']['output']['chart_type']\n",
        "\n",
        "        score = metric_6a(pred_outputs, gt_outputs, gt_type, alpha, beta, gamma, img_dim, debug)\n",
        "        print(score)\n",
        "    elif os.path.isdir(pred_infile) and os.path.isdir(gt_infile):\n",
        "        scores_type = {}\n",
        "        scores = []\n",
        "        for x in os.listdir(pred_infile):\n",
        "            print(\"Processing: %s\" %x)\n",
        "\n",
        "            pred_file = os.path.join(pred_infile, x)\n",
        "            gt_file = os.path.join(gt_infile, x)\n",
        "\n",
        "            try:\n",
        "                pred_json = json.load(open(pred_file))\n",
        "                gt_json = json.load(open(gt_file))\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            pred_outputs = get_dataseries(pred_json)\n",
        "            gt_outputs = get_dataseries(gt_json)\n",
        "            gt_type = gt_json['task1']['output']['chart_type']\n",
        "\n",
        "            score = metric_6a(pred_outputs, gt_outputs, gt_type, alpha, beta, gamma, img_dim, debug)\n",
        "            \n",
        "            if (gt_type in scores_type):\n",
        "                scores_type[gt_type].append(score)\n",
        "            else:\n",
        "                scores_type[gt_type] = [score]\n",
        "            scores.append(score)\n",
        "            print(\"Score: %f\" %score)\n",
        "        avg_score = sum(scores) / len(scores)\n",
        "        print(\"Average Score: %f\" % avg_score)\n",
        "        for types in scores_type:\n",
        "            print(\"Average Score for %s: %f\" %(types, sum(scores_type[types])/len(scores_type[types])))\n",
        "    else:\n",
        "        print(\"Error: pred_file and gt_file must both be files or both be directories\")\n",
        "        exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLBBFjAyihxc"
      },
      "outputs": [],
      "source": [
        "box_detector_folders = [\"horizontal_bar\" ]\n",
        "point_detector_folders = [\"line\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rku1Md2jX51g"
      },
      "outputs": [],
      "source": [
        "calculate_score('/content/drive/MyDrive/pred_output/horizontal_bar','/content/ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21/annotations_JSON/horizontal_bar')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5NAzvnQX54Q"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I--JA4fVMCcO"
      },
      "outputs": [],
      "source": [
        "print(dirName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSFu_5LoMKZH"
      },
      "outputs": [],
      "source": [
        "image_na_split = image_na.split('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4ik70qWMfcK"
      },
      "outputs": [],
      "source": [
        "image_na_split_right = image_na_split[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA6JbYGkMlTA"
      },
      "outputs": [],
      "source": [
        "print(image_na_split_right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ondtZkcli2CE"
      },
      "outputs": [],
      "source": [
        "x = output['instances'].pred_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNKBUeXWsrLY"
      },
      "outputs": [],
      "source": [
        "a=x.tensor.cpu()\n",
        "a=a.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwv38jTYst_6"
      },
      "outputs": [],
      "source": [
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tUsmBB8GdVa"
      },
      "outputs": [],
      "source": [
        "input = {}\n",
        "input['task6'] = {}\n",
        "input['task6']['output'] ={}\n",
        "input['task6']['output']['visual elements'] = {}\n",
        "input['task6']['output']['visual elements']['bars']=[]\n",
        "for i in a:\n",
        "  main_box = {}\n",
        "  main_box['height'] = int(i[3])-int(i[1])\n",
        "  main_box['width'] = int(i[2])-int(i[0])\n",
        "  main_box['x0'] = int(i[0])\n",
        "  main_box['y0'] = int(i[1])\n",
        "  input['task6']['output']['visual elements']['bars'].append(main_box)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZcBa8zAJYmX"
      },
      "outputs": [],
      "source": [
        "print(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R76fjNTnMyvW"
      },
      "outputs": [],
      "source": [
        "final_json = dirName + '/' + image_na_split_right + '.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWPrY0LnM9v8"
      },
      "outputs": [],
      "source": [
        "print(final_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "val6EpKZJeSe"
      },
      "outputs": [],
      "source": [
        "with open('data.json', 'w') as f:\n",
        "    json.dump(input, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_tUQqEh93Py"
      },
      "outputs": [],
      "source": [
        "# input = {}\n",
        "# input['task6'] = {}\n",
        "# input['task6']['output'] ={}\n",
        "# input['task6']['output']['visual elements'] = {}\n",
        "# input['task6']['output']['visual elements']['bars']=[]\n",
        "\n",
        "x = json.dumps(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEDhkKYk93Yp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U4zphjv93ba"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QAdIK-393d3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DF4OPeV93fO"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3gNuFe_93vT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgk2ryvp93wS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCcz34na93xX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gx348CR934-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cUVX5YU9373"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyZTIFJms2oN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXn0UBHToQX5"
      },
      "outputs": [],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL4UyTeJoIsk"
      },
      "outputs": [],
      "source": [
        "print(type(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWnLUhFfoMcw"
      },
      "outputs": [],
      "source": [
        "x.structures.instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJDeZzHxLdfJ"
      },
      "outputs": [],
      "source": [
        "#import the COCO Evaluator to use the COCO Metrics\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "\n",
        "#Call the COCO Evaluator function and pass the Validation Dataset\n",
        "evaluator = COCOEvaluator(\"Chart_Dataset_Val\", cfg, False, output_dir=\"/output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"Chart_Dataset_Val\")\n",
        "\n",
        "#Use the created predicted model in the previous step\n",
        "inference_on_dataset(predictor.model, val_loader, evaluator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5SpePomDFhQ"
      },
      "outputs": [],
      "source": [
        "# def evaluate_model(model,validation_load):\n",
        "#   model.eval()\n",
        "#   main_prediction = 0\n",
        "#   toatal = 0\n",
        "#   device = DEVICE\n",
        "#   with torch.no_grad():\n",
        "#     for i in validation_load:\n",
        "#       images,label = i\n",
        "#       images = images.to(device)\n",
        "#       label = label.to(device)\n",
        "#       total += label.size(0)\n",
        "#       main_o = model(images)\n",
        "      \n",
        "#       _,predicted = torch.max(main_o.i,1)\n",
        "\n",
        "#       ayyd_c  += (predicted==label).sum().item() \n",
        "\n",
        "#     acc = ayyd_c/total\n",
        "\n",
        "#   print(\"Accuracy is {}\".format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELN3ZntPLF2J"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0vZHFC2qG_o"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BFmTCsYqMMs"
      },
      "outputs": [],
      "source": [
        " json_fd = open('/content/ICPR2020_CHARTINFO_UB_PMC_TRAIN_v1.21/annotations_JSON/horizontal_bar/PMC4684910___12889_2015_2565_Fig2_HTML.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6UhbplQqKGd"
      },
      "outputs": [],
      "source": [
        "json_data = json.load(json_fd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq2VhgOaqmJL"
      },
      "outputs": [],
      "source": [
        "if"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lv-RU__qYJW"
      },
      "outputs": [],
      "source": [
        "if('task6' in json_data):\n",
        "  print(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqKZNQowqYXr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ADOBE_SYNTH_CV_main_project_detectron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}